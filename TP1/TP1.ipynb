{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Overview\n",
    "\n",
    "Twitter is a mix of social network and microblogging. In this network, people post information and communicate among themselves through messages, called tweets, that can contain up to 280 characters. In this assignment, *we will implement a prototype that can detect if an airline company is positively or negatively mentioned in a tweet*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# 2 - Sentiment Analysis Model (13 points)\n",
    "\n",
    "In the literature, the task of extracting the sentiment of a text is called *sentiment analysis*. We will implement a bag-of-words (BoW) model for this task.\n",
    "\n",
    "## 2.1 -  Setup\n",
    "\n",
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/gregoire/.local/lib/python3.6/site-packages (19.1.1)\n",
      "Requirement already satisfied: numpy in /home/gregoire/.local/lib/python3.6/site-packages (1.16.1)\n",
      "Requirement already satisfied: sklearn in /home/gregoire/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/gregoire/.local/lib/python3.6/site-packages (from sklearn) (0.20.3)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/gregoire/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.16.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/gregoire/.local/lib/python3.6/site-packages (from scipy) (1.16.1)\n",
      "Requirement already satisfied: nltk in /home/gregoire/.local/lib/python3.6/site-packages (3.4.1)\n",
      "Requirement already satisfied: six in /home/gregoire/.local/lib/python3.6/site-packages (from nltk) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gregoire/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gregoire/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gregoire/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/gregoire/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "!pip install --upgrade --user pip\n",
    "!pip install --user numpy\n",
    "!pip install --user sklearn\n",
    "!pip install --user scipy\n",
    "!pip install --user nltk\n",
    "\n",
    "\n",
    "#python\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Dataset\n",
    "\n",
    "Please download the zip file in the following url: https://drive.google.com/file/d/1iGmESwPXpO3sIZFGOCrysxJ27AHdly-Y/view?usp=sharing\n",
    "\n",
    "In this zip file, there are 2 files:\n",
    "1. train.tsv: training dataset\n",
    "2. dev.tsv: validation dataset\n",
    "\n",
    "Each line of the files has the following information about a tweet: *tweet id*, *user id*, *label* and *message text*.\n",
    "\n",
    "There are three labels in the dataset: *negative*, *neutral* and *positive*. We represent each one of these labels as 0, 1 and 2 respectively.\n",
    "\n",
    "In the code above read the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import math\n",
    "\n",
    "def load_dataset(path):\n",
    "    dtFile = codecs.open(path, 'r')\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for l in dtFile:\n",
    "        sid, uid, label,text = re.split(r\"\\s+\", l, maxsplit=3)\n",
    "        \n",
    "        text = text.strip()\n",
    "        \n",
    "        # Remove not available\n",
    "        if text == \"Not Available\":\n",
    "            continue\n",
    "        \n",
    "        x.append(text)\n",
    "        \n",
    "        if label == \"negative\": \n",
    "            y.append(0)\n",
    "        elif label == \"neutral\": \n",
    "            y.append(1)\n",
    "        elif label == \"positive\": \n",
    "            y.append(2)\n",
    "        \n",
    "    assert len(x) == len(y)\n",
    "            \n",
    "    return x,y\n",
    "            \n",
    "\n",
    "# Path of training dataset\n",
    "trainingPath=\"./train_data.tsv\"\n",
    "\n",
    "# Path of validation dataset\n",
    "validationPath=\"./dev_data.tsv\"\n",
    "\n",
    "training_X, training_Y = load_dataset(trainingPath)\n",
    "validation_X, validation_Y = load_dataset(validationPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "### 2.3.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). For instance, the sentence *\"It's the student's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "#### 2.3.1.1 - Question 1 (0.5 point) \n",
    "\n",
    "Implement the SpaceTokenizer and NLTKTokenizer tokenizers: \n",
    "- **SpaceTokenizer** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **NLTKTokenizer** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "\n",
    "**All tokenizers have to lowercase the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceTokenizer(object):\n",
    "    \"\"\"\n",
    "    It tokenizes the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        try:\n",
    "            tokens = text.split()\n",
    "        except:\n",
    "            raise NotImplementedError(\"space tokenize error\")\n",
    "        return tokens\n",
    "        \n",
    "class NLTKTokenizer(object):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "        except:\n",
    "            raise NotImplementedError(\"nltk tokenize error\")\n",
    "        \n",
    "        # Have to return a list of tokens\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'suis', 'pas', 'bo']\n",
      "['je', 'suis', 'pas', 'bo']\n"
     ]
    }
   ],
   "source": [
    "tk = NLTKTokenizer()\n",
    "print(tk.tokenize(\"je suis pas bo\"))\n",
    "tk = SpaceTokenizer()\n",
    "print(tk.tokenize(\"je suis pas bo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 - Stemming\n",
    "\n",
    "In the tweets *\"I should have bought a new shoes today\"* and *\"I spent too much money buying games\"*, the words *\"buy\"* and *\"bought\"* represent basically the same concept. Considering both words as different can unnecessarily increase the dimensionality of the problem and can negatively impact the performance of simple models. Therefore, a unique form (e.g., the root buy) can represent both words. The process to convert words with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*.\n",
    "\n",
    "#### 2.3.2.1 - Question 2 (0.5 point) \n",
    "\n",
    "Retrieve the stems of the tokens using the attribute *stemmer* from the class *Stemmer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "class Stemmer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "    def stem(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: a list of strings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        except:\n",
    "            raise NotImplementedError(\"\")\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buy', 'bought', 'run', 'was', 'report', 'generous']\n"
     ]
    }
   ],
   "source": [
    "st = Stemmer().stemmer\n",
    "words = ['buying', 'bought', 'running', 'was', 'reported','generously']\n",
    "output = [st.stem(word) for word in words]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 - Twitter preprocessing\n",
    "\n",
    "Sometimes only applying the default NLP preprocessing steps is not enough. Data for certain domains can have peculiar characteristics which requires specific preprocessing steps to remove the noise and create a more suitable format for the models. \n",
    "\n",
    "In NLP, methods store a set of words, called dictionary, and all the words out of the dictionary are considered as unknown. In this assignment, the feature space dimensionality of a model is directly related to the number of words in the dictionary. Since high-dimensional spaces can suffer from the curse of dimensionality, our goal is to create preprocessing steps that decrease vocabulary size.  \n",
    "\n",
    "#### 2.3.3.1 - Question 3 (2.0 points)\n",
    "\n",
    "Briefly explain and implement at least two preprocessing steps that reduce the dictionary size (number of unique words). These preprocessing steps must be related to the specific characteristic of the Twitter data. Therefore, for instance, the stop words removal will not be accepted as a preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class TwitterPreprocessing(object):\n",
    "    \n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: original tweet\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if tweet == \"Not Available\":\n",
    "                return tweet\n",
    "            # on retire les mots qui commencent par des @ (tags) et http (liens)\n",
    "            tweet = re.sub(\"(http|@)[^ \\t\\n\\r]*([\\s]|$)\", \"\", tweet)\n",
    "            # on retire les #\n",
    "            tweet = re.sub(\"#\", \"\", tweet)\n",
    "            # on remplace les chiffres par des espaces\n",
    "            tweet = re.sub(\"[0-9]\", \" \", tweet)\n",
    "            \n",
    "        except:\n",
    "            raise NotImplementedError(\"\")\n",
    "    \n",
    "        # return the preprocessed twitter\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coucou gregoire hello  \n"
     ]
    }
   ],
   "source": [
    "prepro = TwitterPreprocessing()\n",
    "print(prepro.preprocess(\"@juju #coucou https://moodle.polymtl.ca gregoire hello2 @coucou https://moodle.polymtl.ca\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3  Pipeline\n",
    "\n",
    "The pipeline is sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. We implement the class *PreprocessingPipeline* that apply the tokenizer, twitter preprocessing and stemer to the text.\n",
    "\n",
    "**Feel free to change the preprocessing order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    \n",
    "    def __init__(self, tokenization, twitterPreprocessing, stemming):\n",
    "        \"\"\"\n",
    "        tokenization: enable or disable tokenization.\n",
    "        twitterPreprocessing: enable or disable twitter preprocessing.\n",
    "        stemming: enable or disable stemming.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer= NLTKTokenizer() if tokenization else SpaceTokenizer()\n",
    "        self.twitterPreprocesser = TwitterPreprocessing() if twitterPreprocessing else None\n",
    "        self.stemmer = Stemmer() if stemming else None\n",
    "    \n",
    "    def preprocess(self, tweet):\n",
    "        \"\"\"\n",
    "        Transform the raw data\n",
    "\n",
    "        tokenization: boolean value.\n",
    "        twitterPreprocessing: boolean value. Apply the\n",
    "        stemming: boolean value.\n",
    "        \"\"\"\n",
    "        if self.twitterPreprocesser:\n",
    "            tweet = self.twitterPreprocesser.preprocess(tweet)\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(tweet)\n",
    "\n",
    "        if self.stemmer:\n",
    "            tokens = self.stemmer.stem(tokens)\n",
    "            \n",
    "        \n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_pipe = PreprocessingPipeline(True, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 N-grams\n",
    "\n",
    "An n-gram is a contiguous sequence of *n* tokens from a text. Thus, for instance,the sequence *\"bye as\"* and *\"walked through\"* are example of 2-grams from the sentence *\"He said bye as he walked through the door .\"*. 1-gram, 2-gram and 3-gram are, respectively, called unigram, bigram and trigram. We list all the possible unigram, bigram and trigram from the *\"He said bye as he walked through the door .\"*:\n",
    "\n",
    "- Unigram: [\"He\", \"said\", \"bye\", \"as\", \"he\", \"walked\", \"through\", \"the\", \"door\", \".\"]\n",
    "- Bigram: [\"He said\", \"said bye\", \"bye as\", \"as he\", \"he walked\", \"walked through\", \"through the\", \"the door\", \"door .\"] \n",
    "- Trigram: [\"He said bye\", \"said bye as\", \"bye as he\", \"as he walked\", \"he walked through\", \"walked through the\", \"through the door\", \"the door .\"] \n",
    "\n",
    "\n",
    "### 2.4.1 - Question 4 (1 point)\n",
    "\n",
    "Implement bigram and trigram.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    len_tokens = len(tokens)\n",
    "    bigram = []\n",
    "    i = 0\n",
    "    while i < len_tokens - 1:\n",
    "        bigram.append(tokens[i] + ' ' + tokens[i + 1])\n",
    "        i+=1\n",
    "    return bigram\n",
    "        \n",
    "    \n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    len_tokens = len(tokens)\n",
    "    trigram = []\n",
    "    i = 0\n",
    "    while i < len_tokens - 2:\n",
    "        trigram.append(tokens[i] + ' ' + tokens[i + 1] + ' ' + tokens[i + 2])\n",
    "        i+=1\n",
    "    return trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coucou la', 'la compagnie', 'compagnie !']\n",
      "['coucou la compagnie', 'la compagnie !']\n"
     ]
    }
   ],
   "source": [
    "print(bigram(['coucou', 'la', 'compagnie', '!']))\n",
    "print(trigram(['coucou', 'la', 'compagnie', '!']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bag-of-words\n",
    "\n",
    "Logistic regression, SVM and other well-known models only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two tweets: ”Board games are much better than video games” and ”Pandemic is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | pandemic | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "### 2.5.1 - Question 5 (2 points)\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.tokens_index = []\n",
    "\n",
    "    def get_tokens_list(self, tokens):\n",
    "        tokens_list = tokens\n",
    "        if self.bigram: tokens_list += bigram(tokens)\n",
    "        if self.trigram: tokens_list += trigram(tokens)\n",
    "        return tokens_list\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object, relates each unigram, bigram or trigram to a specific integer and\n",
    "        transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "        X: a list that contains tweet contents\n",
    "\n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens_index = []\n",
    "        fited_tweets = [[]]\n",
    "        try:\n",
    "            for x in X:\n",
    "                if x != \"Not Available\":\n",
    "                    tokens = self.pipeline.preprocess(x)\n",
    "                    tokens_list = self.get_tokens_list(tokens)\n",
    "                    new_fited_tweet = [0 for i in range(len(fited_tweets[0]))]\n",
    "                    for token in tokens_list:\n",
    "                        if token in self.tokens_index:\n",
    "                            new_fited_tweet[self.tokens_index.index(token)] += 1\n",
    "                        else:\n",
    "                            for fited_tweet in fited_tweets:\n",
    "                                fited_tweet.append(0)\n",
    "                            new_fited_tweet.append(1)\n",
    "                            self.tokens_index.append(token)\n",
    "                    fited_tweets.append(new_fited_tweet)\n",
    "        except:\n",
    "            raise NotImplementedError(\"\")\n",
    "\n",
    "        return csr_matrix(fited_tweets[1:])\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object and  transforms the text in a list of integer.\n",
    "        Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "        X: a list of vectors\n",
    "\n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "        fited_tweets = csr_matrix((len(X), len(self.tokens_index)), dtype=np.int8)\n",
    "        try:\n",
    "            index = 0\n",
    "            for x in X:\n",
    "                tokens = self.pipeline.preprocess(x)\n",
    "                tokens_list = self.get_tokens_list(tokens)\n",
    "                for token in tokens_list:\n",
    "                    if token in self.tokens_index:\n",
    "                        fited_tweets[index, self.tokens_index.index(token)] += 1\n",
    "                index += 1\n",
    "        except:\n",
    "            raise NotImplementedError(\"\")\n",
    "\n",
    "        return csr_matrix(fited_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0 0 0 0 0]\n",
      " [1 0 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 0 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "cb = CountBoW(pre_process_pipe, False, False)\n",
    "print(cb.fit_transform([\"coucou papa !\", \"coucou maman !\",\"coucou papa ! salut la compagnie ca va ,\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0 0 0 0 0]\n",
      " [1 0 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 0 1 1 1 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregoire/.local/lib/python3.6/site-packages/ipykernel_launcher.py:72: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "print(cb.transform([\"coucou papa !\", \"coucou maman !\",\"coucou papa ! salut la compagnie ca va ,\"]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 - TF-IDF\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of tweets. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\tidf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the dataset, $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "\tw_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "where $tf_{ij}$ is the term frequency of word $i$ in the document $j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.5.2.1 - Question 6 (3 points)\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g., scikit-learn). However, if you have a problem with memory size, you can use the class scipy.sparse.csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFBoW(CountBoW):\n",
    "\n",
    "    def __init__(self, pipeline, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        pipelineObj: instance of PreprocesingPipeline\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.count_bow = CountBoW(pipeline, False, False)\n",
    "        self.idf = []\n",
    "        self.tokens_index = []\n",
    "\n",
    "    def get_idf(self, bow_matrix):\n",
    "        \"\"\"\n",
    "        this methode return the idf of the set of tokens\n",
    "\n",
    "        bow_matrix: the matrix of the raw frequency\n",
    "        :return: a list of the idf by index\n",
    "        \"\"\"\n",
    "\n",
    "        idf = []\n",
    "        for i in range(len(self.tokens_index)):\n",
    "            dfi = 0\n",
    "            for j in range(len(bow_matrix)):\n",
    "                # on compte le nombre de documents qui contiennent ce mot\n",
    "                dfi += 1 if bow_matrix[j][i] != 0 else 0\n",
    "            # on calcul et ajoute le idf a la liste des idf\n",
    "            idf.append(math.log(len(bow_matrix) / dfi))\n",
    "        return idf\n",
    "\n",
    "    def get_weighted_matrix(self, bow_matrix, save_idf = False):\n",
    "        \"\"\"\n",
    "        this methode return the new matrix by tf-idf\n",
    "\n",
    "        bow_matrix: the matrix of the raw frequency\n",
    "        :return: the new matrix weigted\n",
    "        \"\"\"\n",
    "\n",
    "        idf = self.get_idf(bow_matrix) if save_idf else self.idf\n",
    "        if save_idf: self.idf = idf\n",
    "        for j in range(len(bow_matrix)):\n",
    "            for i in range(len(self.tokens_index)):\n",
    "                bow_matrix[j][i] = bow_matrix[j][i] * idf[i]\n",
    "        return bow_matrix\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object, calculates the IDF and TF and\n",
    "        transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "\n",
    "        X: a list that contains tweet contents\n",
    "\n",
    "        :return: a list that contains the list of integers\n",
    "        \"\"\"\n",
    "        self.tokens_index = []\n",
    "        fited_tweets = [[]]\n",
    "        try:\n",
    "            for x in X:\n",
    "                if x != \"Not Available\":\n",
    "                    tokens = self.pipeline.preprocess(x)\n",
    "                    tokens_list = self.get_tokens_list(tokens)\n",
    "                    new_fited_tweet = [0 for i in range(len(fited_tweets[0]))]\n",
    "                    for token in tokens_list:\n",
    "                        if token in self.tokens_index:\n",
    "                            new_fited_tweet[self.tokens_index.index(token)] += 1\n",
    "                        else:\n",
    "                            for fited_tweet in fited_tweets:\n",
    "                                fited_tweet.append(0)\n",
    "                            new_fited_tweet.append(1)\n",
    "                            self.tokens_index.append(token)\n",
    "                    fited_tweets.append(new_fited_tweet)\n",
    "        except:\n",
    "            raise NotImplementedError(\"\")\n",
    "\n",
    "        return csr_matrix(self.get_weighted_matrix(fited_tweets[1:], True))\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This method preprocesses the data using the pipeline object and\n",
    "            transforms the text in a list of integer.\n",
    "\n",
    "        X: a list of vectors\n",
    "\n",
    "        :return: a list of vectors\n",
    "        \"\"\"\n",
    "        fited_tweets = csr_matrix((len(X), len(self.tokens_index)), dtype=np.int8)\n",
    "\n",
    "        try:\n",
    "            index = 0\n",
    "            for x in X:\n",
    "                tokens = self.pipeline.preprocess(x)\n",
    "                tokens_list = self.get_tokens_list(tokens)\n",
    "                for token in tokens_list:\n",
    "                    if token in self.tokens_index:\n",
    "                        fited_tweets[index, self.tokens_index.index(token)] += 1\n",
    "                index += 1\n",
    "\n",
    "        except:\n",
    "            raise NotImplementedError(\"\")\n",
    "\n",
    "        return self.get_weighted_matrix(fited_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.40546511 0.         0.40546511 0.40546511 0.40546511\n",
      "  1.09861229 1.09861229 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.09861229 1.09861229 1.09861229 1.09861229\n",
      "  1.09861229 1.09861229 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.40546511 0.         0.40546511 0.40546511 0.40546511\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.09861229 1.09861229 1.09861229 1.09861229\n",
      "  1.09861229 1.09861229 1.09861229 1.09861229 1.09861229 1.09861229\n",
      "  1.09861229 1.09861229 1.09861229 1.09861229 1.09861229 1.09861229\n",
      "  1.09861229 1.09861229 1.09861229 1.09861229 1.09861229 1.09861229\n",
      "  1.09861229 1.09861229 1.09861229 1.09861229]]\n"
     ]
    }
   ],
   "source": [
    "cb = TFIDFBoW(pre_process_pipe, True, True)\n",
    "print(cb.fit_transform([\"coucou papa !\", \"coucou maman !\",\"coucou papa ! salut la compagnie ca va ?\"]).toarray())\n",
    "\n",
    "#str_test = \"Je m'appelle Elisa day?tapos Elisa\"\n",
    "#str_test2 = \"Je vais voir Elisa au restaurant\"\n",
    "#str_test3 = \"Je vais au Supermarché\"\n",
    "#X = [str_test, str_test2, str_test3]\n",
    "#print(\"\\n\")\n",
    "#print(cb.fit_transform(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Classifier using BoW\n",
    "\n",
    "We are going to use logistic regression as a classifier. Read the following page to now more about this classifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "\n",
    "The method *train_evaluate* trains and evaluates the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_evaluate(training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "\n",
    "    bowObj: Bag-of-word object\n",
    "\n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "\n",
    "    debut_fit_trans = time.time()\n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "    fin_fit_trans = time.time() - debut_fit_trans\n",
    "\n",
    "    print(\"     taille dico: \" + str(training_rep.shape[1]) + \" nb tweet: \" + str(training_rep.shape[0]))\n",
    "\n",
    "    debut_train = time.time()\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "    fin_train = time.time() - debut_train\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "\n",
    "    debut_valid = time.time()\n",
    "    validationAcc = accuracy_score(validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
    "    fin_valid = time.time() - debut_valid\n",
    "\n",
    "    print(\"     temps: fit_trans: {}s training: {}s validation: {}s\".format(fin_fit_trans, fin_train, fin_valid))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: trainX: 6817, TrainY: 6817, validX: 1133, validY: 1133\n"
     ]
    }
   ],
   "source": [
    "print(\"data shape: trainX: {}, TrainY: {}, validX: {}, validY: {}\".format(len(training_X),\n",
    "                                                                         len(training_Y),\n",
    "                                                                         len(validation_X),\n",
    "                                                                         len(validation_Y)))\n",
    "\n",
    "train_evaluate(training_X, training_Y, validation_X, validation_Y, cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.6.1 - Question 7 (4 points)\n",
    "\n",
    "Train and calculate the logistic regression accuracy in the *training and validation dataset* using each one of the following configurations:\n",
    "    1. CountBoW + SpaceTokenizer(without tokenizer) + unigram \n",
    "    2. CountBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + unigram\n",
    "    3. TFIDFBoW + NLTKTokenizer + Stemming + unigram\n",
    "    4. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\n",
    "    5. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\n",
    "    6. TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\n",
    "Besides the accuracy, you have to report the dictionary size for each one of configurations. Finally, describe the results found by you and answer the following questions:\n",
    "- Which preprocessing has helped the model? Why?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred? \n",
    "- Has the bigram and trigram improved the performance? If yes, can you mention the reasons of this improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def custom_evaluate(training_X, training_Y, validation_X, validation_Y, options):\n",
    "    \"\"\"\n",
    "        training_X: tweets from the training dataset\n",
    "        training_Y: tweet labels from the training dataset\n",
    "        validation_X: tweets from the validation dataset\n",
    "        validation_Y: tweet labels from the validation dataset\n",
    "        options: options on the programme\n",
    "\n",
    "        :return: the classifier and its accuracy in the training and validation dataset.\n",
    "        \"\"\"\n",
    "    pre_process_pipe = PreprocessingPipeline(options[\"tokenisation\"],\n",
    "                                             options[\"preprocess\"],\n",
    "                                             options[\"stemming\"])\n",
    "    if options[\"Bow_methode\"] == \"CountBow\":\n",
    "        bow = CountBoW(pre_process_pipe, options[\"bigram\"], options[\"trigram\"])\n",
    "    else:\n",
    "        bow = TFIDFBoW(pre_process_pipe, options[\"bigram\"], options[\"trigram\"])\n",
    "\n",
    "    classifier = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "\n",
    "    debut_fit_trans = time.time()\n",
    "    training_rep = bow.fit_transform(training_X)\n",
    "    fin_fit_trans = time.time() - debut_fit_trans\n",
    "\n",
    "    print(\"     taille dico: \" + str(training_rep.shape[1]) + \" nb tweet: \" + str(training_rep.shape[0]))\n",
    "\n",
    "    debut_train = time.time()\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "    fin_train = time.time() - debut_train\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "\n",
    "    debut_valid = time.time()\n",
    "    validationAcc = accuracy_score(validation_Y, classifier.predict(bow.transform(validation_X)))\n",
    "    fin_valid = time.time() - debut_valid\n",
    "\n",
    "    print(\"     temps: fit_trans: {}s training: {}s validation: {}s\".format(fin_fit_trans, fin_train, fin_valid))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc\n",
    "\n",
    "def test_model(ls_options):\n",
    "\n",
    "    for option in ls_options:\n",
    "        try:\n",
    "            print(\"\\n\" + option[\"name\"] + \" ----\\n\")\n",
    "            _, trainAcc, validationAcc = custom_evaluate(training_X, training_Y, validation_X, validation_Y, option)\n",
    "            print(\"\\n     trainAcc: \" + str(trainAcc))\n",
    "            print(\"     validationAcc: \" + str(validationAcc))\n",
    "\n",
    "        except MemoryError:\n",
    "            print(\"not enough memory for this one\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Path of training dataset\n",
    "    trainingPath = \"./train_data.tsv\"\n",
    "\n",
    "    # Path of validation dataset\n",
    "    validationPath = \"./dev_data.tsv\"\n",
    "\n",
    "    training_X, training_Y = load_dataset(trainingPath)\n",
    "    validation_X, validation_Y = load_dataset(validationPath)\n",
    "    pre_process_pipe = PreprocessingPipeline(True, True, True)\n",
    "\n",
    "    bow = CountBoW(pre_process_pipe, False, False)\n",
    "\n",
    "    test_model([\n",
    "        {\n",
    "            \"name\": \"CountBoW + SpaceTokenizer(without tokenizer) + unigram\",\n",
    "            \"Bow_methode\": \"CountBow\",\n",
    "            \"tokenisation\": False,\n",
    "            \"preprocess\": False,\n",
    "            \"stemming\": False,\n",
    "            \"bigram\": False,\n",
    "            \"trigram\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CountBoW + SpaceTokenizer(without tokenizer) + unigram + bigram\",\n",
    "            \"Bow_methode\": \"CountBow\",\n",
    "            \"tokenisation\": False,\n",
    "            \"preprocess\": False,\n",
    "            \"stemming\": False,\n",
    "            \"bigram\": True,\n",
    "            \"trigram\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CountBoW + NLTKTokenizer + unigram\",\n",
    "            \"Bow_methode\": \"CountBow\",\n",
    "            \"tokenisation\": True,\n",
    "            \"preprocess\": False,\n",
    "            \"stemming\": False,\n",
    "            \"bigram\": False,\n",
    "            \"trigram\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TFIDFBoW + NLTKTokenizer + unigram\",\n",
    "            \"Bow_methode\": \"TFIDFBoW\",\n",
    "            \"tokenisation\": True,\n",
    "            \"preprocess\": False,\n",
    "            \"stemming\": False,\n",
    "            \"bigram\": False,\n",
    "            \"trigram\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TFIDFBoW + NLTKTokenizer + Stemming + unigram\",\n",
    "            \"Bow_methode\": \"TFIDFBoW\",\n",
    "            \"tokenisation\": True,\n",
    "            \"preprocess\": False,\n",
    "            \"stemming\": True,\n",
    "            \"bigram\": False,\n",
    "            \"trigram\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram\",\n",
    "            \"Bow_methode\": \"TFIDFBoW\",\n",
    "            \"tokenisation\": True,\n",
    "            \"preprocess\": True,\n",
    "            \"stemming\": True,\n",
    "            \"bigram\": False,\n",
    "            \"trigram\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram\",\n",
    "            \"Bow_methode\": \"TFIDFBoW\",\n",
    "            \"tokenisation\": True,\n",
    "            \"preprocess\": True,\n",
    "            \"stemming\": True,\n",
    "            \"bigram\": True,\n",
    "            \"trigram\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TFIDFBoW + NLTKTokenizer + Twitter preprocessing + Stemming  + unigram + bigram + trigram\",\n",
    "            \"Bow_methode\": \"TFIDFBoW\",\n",
    "            \"tokenisation\": True,\n",
    "            \"preprocess\": True,\n",
    "            \"stemming\": True,\n",
    "            \"bigram\": True,\n",
    "            \"trigram\": True\n",
    "        },\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Prototype (7 points)\n",
    "\n",
    "During the last years, *E Corp* has collected tweets to create a dataset to their sentiment analysis tool. Now, airline companies have contracted *E Corp* to analyze the consumer opinion about them. Your job is to extract information from the tweet database about the following companies: Air France, American, British Airways,  Delta, Southwest, United, Us Airways and Virgin America.\n",
    "\n",
    "*For the prototype, you have to use the best model found in the Section 2.*\n",
    "\n",
    "## 3.1 Dataset\n",
    "\n",
    "In https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing, you can find the raw tweet retrieved by E corp.  Each tweet is represented as json that the have attributes listed in the page https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.\n",
    "\n",
    "** You will answer the question of this section using this tweet database (https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing).**\n",
    "\n",
    "## 3.2 Sentiment Analysis\n",
    "\n",
    "\n",
    "### 3.2.1 Question 8 (0.5 point)\n",
    "\n",
    "Implement the method *extract_tweet_content* that extracts the content of each tweet in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweet_content(raw_tweet_file):\n",
    "    \"\"\"\n",
    "    Extract the tweet content for each json object\n",
    "\n",
    "    raw_tweet_file: file path that contains all json objects\n",
    "\n",
    "    :return: a list with the tweet contents\n",
    "    \"\"\"\n",
    "    tweet_list = []\n",
    "    try:\n",
    "        with open(\"e_corp_dataset.txt\", \"r\") as reader:\n",
    "            for line in reader:\n",
    "                json_line = json.load(line)\n",
    "                tweet_list.append(json_line)\n",
    "    except:\n",
    "        raise NotImplementedError(\"\")\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Question 9 (1 points)\n",
    "\n",
    "Implement the method *detect_airline* that detects the airline companies in a tweet. Besides that, explain your approach to detect the companies and its possible drawbacks.\n",
    "\n",
    "The detect_airline has to be able to return if none or more than one airline companies are mentioned in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_airline(tweet_msg):\n",
    "    \"\"\"\n",
    "    Detect and return the airline companies mentioned in the tweet\n",
    "\n",
    "    tweet_msg: represents the tweet message.\n",
    "\n",
    "    :return: list of detected airline companies\n",
    "    \"\"\"\n",
    "\n",
    "    present_compagnies = []\n",
    "    try:\n",
    "        compagnies = \"([aA]ir\\s[fF]rance|\" \\\n",
    "                     \"[aA]merican\\s[aA]irways|\" \\\n",
    "                     \"[bB]ritish\\s[aA]irways|\" \\\n",
    "                     \"[dD]elta|\" \\\n",
    "                     \"[S]outhwest|\" \\\n",
    "                     \"[uU]nited|\" \\\n",
    "                     \"[uU]s\\[aA]irways|\" \\\n",
    "                     \"[vV]irgin\\s[aA]merica)\"\n",
    "        if re.match(r'.+' + compagnies, tweet_msg):\n",
    "            present_compagnies.append(tweet_msg)\n",
    "\n",
    "    except:\n",
    "        raise NotImplementedError(\"\")\n",
    "\n",
    "    return present_compagnies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.1 Question 10 (0.5 points)\n",
    "\n",
    "Implement the method *extract_sentiment* that receives a tweet and extracts its sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(classifier, tweet):\n",
    "    \"\"\"\n",
    "    Extract the tweet sentiment\n",
    "    \n",
    "    classifier: classifier object\n",
    "    tweet: represents the tweet message, as a row of the BoW matrix\n",
    "    \n",
    "    :return: list of detected airline companies\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return classifier.predict(tweet)\n",
    "    except:\n",
    "        raise NotImplementedError(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Question 11 (2 points)\n",
    "\n",
    "Using the *extract_tweet_content*, *detect_airline* and *extract_sentiment*, implement a code that generates a bar chart that contains the number of positive, neutral and negatives tweets for each one of the companies. Briefly describe your bar chart (e.g, which was the company with most negative tweets) and how this chart can help airline companies.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Term Analysis\n",
    "\n",
    "POS-tagging consists of extracting the part-of-speech (POS) of each token in a sentence. For instance, the table below depicts the part-of-speechs of the sentence *The cat is white!* are.\n",
    "\n",
    "\n",
    "\n",
    "|   The   | cat  |  is  | white     |    !       |\n",
    "|---------|------|------|-----------|------------|\n",
    "| article | noun | verb | adjective | punctation |\n",
    "\n",
    "\n",
    "The part-of-speech can be more complex than what we have learned in the school. Linguistics need to have a more detailed information about systax information of the words in a sentence. For our problem, we do not need this level of information and, thus, we will use a less complex set, called universal POS tags. \n",
    "\n",
    "In POS-tagging, each part-of-speech is represented by a tag. You can find the POS tag list used in this assignement at https://universaldependencies.org/u/pos/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK POS-tagger\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "#before using pos_tag function, you have to tokenize the sentence.\n",
    "s = ['The', 'cat', 'is',  'white', '!']\n",
    "nltk.pos_tag(s,tagset='universal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Question 12 (2 points)\n",
    "\n",
    "**Implement a code** that retrieves the top 10 most frequent terms for each airline company. You will only consider the terms that appear in a positive and negative tweets. Besides that, we consider as term:\n",
    "1. Words that are either an adjective or a noun\n",
    "2. n-grams that are composed by adjectives followed by a noun (e.g., dirty place) or a noun followed by another noun (e.g.,sports club).\n",
    "\n",
    "Moreover, **generate a table** with the top 10 most frequent terms and their normalized frequencies(percentage) for each airline company.\n",
    "\n",
    "**Do not forget to remove the company names from the chart.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Question 13 (1 point)\n",
    "\n",
    "The table generated in the Question 12 can lead us to any conclusion about each one of the 9 companies? Can we identify specific events that have occured during the data retrieval?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Bonus (2 points)\n",
    "\n",
    "Person names, companies names and locations are called named entities. Named-entity recognition (NER) is the task of extracting named entities  classifying them using pre-defined categories. In this bonus section, you will use a Named Entity Recognizer to automatically extract named entities from the tweets. This approach is generic enough to retrieve information about other companies or even product and people names.\n",
    "\n",
    "**For the bonus, you are free to use any Named Entity Recognizer that has python wrapper or is implemented in python. Moreover, you have to use the tweet database of the previous section (https://drive.google.com/file/d/1Cuw6Y12Bj91vF_iH49mqPZZfJkY92iBY/view?usp=sharing)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Bonus 2 (1 point)\n",
    "\n",
    "Implement a code that generates the table with the top 10 most mentioned named entities in the database (this table has to contain the frequencies of the name entities). After that, generates a bar chart that despicts the number of positive, negative and neutral tweets for each one of these 10 named entities. Briefly describe the results found in the bar chart.\n",
    "\n",
    "*Ignore the named entities related to the following airline companies : Air France, American, British Airways,  Delta, Southwest, United, Us Airways and Virgin America.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Bonus 3 (1 point)\n",
    "\n",
    "Generate a similar table produced in the Question 12 for the 10 most mentioned named entities in Bonus 2. Can we draw any conclusion about these named entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
